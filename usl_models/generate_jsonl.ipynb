{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 22:43:52.991591: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-12 22:43:52.991637: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-12 22:43:52.993044: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-12 22:43:53.001284: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-12 22:43:55.205858: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 22:43:55.216974: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 22:43:55.220263: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 22:43:55.335349: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 22:43:55.338796: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 22:43:55.341886: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 22:43:55.656595: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 22:43:55.658784: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 22:43:55.660795: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-07-12 22:43:55.660856: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-12 22:43:55.662881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset form firestore\n",
    "# Setup with GPU\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "import json\n",
    "from usl_models.flood_ml.trainingdataset import IncrementalTrainDataGenerator\n",
    "\n",
    "sim_names = [\"Manhattan-config_v1%2FRainfall_Data_1.txt\"]\n",
    "\n",
    "generator = IncrementalTrainDataGenerator()\n",
    "dataset = generator.load_dataset_windowed(sim_names, batch_size=0, max_chunks=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature and label tensors...\n",
      "Fetching rainfall configuration...\n",
      "Configuration:  config_v1%2FRainfall_Data_1.txt\n",
      "Rainfall duration:  19\n",
      "The output label tensor will initially be of shape:  [1000, 1000, 19]\n",
      "Creating feature chunks...\n",
      "Creating label chunks...\n",
      "Length of common indices: 66\n",
      "Feature file: gs://test-climateiq-study-area-feature-chunks/Manhattan/scaled_chunk_1_3.npy\n",
      "Label file: gs://test-climateiq-study-area-label-chunks/Manhattan/config_v1/Rainfall_Data_1.txt/1_3.npy\n",
      "Finished creating feature and label tensors...\n",
      "Feature tensor shape: (1000, 1000, 8)\n",
      "Label tensor shape: (19, 1000, 1000)\n",
      "\n",
      "\n",
      "Generating temporal tensors...\n",
      "Creating temporal chunks...\n",
      "Configuration:  config_v1%2FRainfall_Data_1.txt\n",
      "Fetching document: config_v1%2FRainfall_Data_1.txt\n",
      "Loading temporal tensor from gs://test-climateiq-study-area-feature-chunks/rainfall/config_v1/Rainfall_Data_1.npy...\n",
      "Temporal tensor shape:  (864, 6)\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1000, 1000, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/var/tmp/ipykernel_49497/1827984150.py\", line 8, in serialize_example  *\n        example = tf.train.Example(features = tf.train.Features(\n\n    TypeError: <tf.Tensor 'SerializeTensor:0' shape=() dtype=string> has type SymbolicTensor, but expected one of: bytes\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m   example \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mExample(features \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mFeatures(\n\u001b[1;32m      9\u001b[0m     feature\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     10\u001b[0m       k: tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mFeature(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     14\u001b[0m   ))\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m example\u001b[38;5;241m.\u001b[39mSerializeToString()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mserialize_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/tmp/__autograph_generated_fileqw7cy4t6.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__serialize_example\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     12\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     13\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspatiotemporal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 14\u001b[0m example \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mExample, (), \u001b[38;5;28mdict\u001b[39m(features\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mFeatures, (), \u001b[38;5;28mdict\u001b[39m(feature\u001b[38;5;241m=\u001b[39m{ag__\u001b[38;5;241m.\u001b[39mld(k): ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mFeature, (), \u001b[38;5;28mdict\u001b[39m(bytes_list\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mBytesList, (), \u001b[38;5;28mdict\u001b[39m(value\u001b[38;5;241m=\u001b[39m[ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mserialize_tensor, (ag__\u001b[38;5;241m.\u001b[39mld(v),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)]), fscope)), fscope) \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(inputs)\u001b[38;5;241m.\u001b[39mitems, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)}), fscope)), fscope)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/var/tmp/__autograph_generated_fileqw7cy4t6.py:14\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     13\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspatiotemporal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 14\u001b[0m example \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mExample, (), \u001b[38;5;28mdict\u001b[39m(features\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mFeatures, (), \u001b[38;5;28mdict\u001b[39m(feature\u001b[38;5;241m=\u001b[39m{ag__\u001b[38;5;241m.\u001b[39mld(k): ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mFeature, (), \u001b[38;5;28mdict\u001b[39m(bytes_list\u001b[38;5;241m=\u001b[39m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesList\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m), fscope) \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(inputs)\u001b[38;5;241m.\u001b[39mitems, (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)}), fscope)), fscope)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/var/tmp/ipykernel_49497/1827984150.py\", line 8, in serialize_example  *\n        example = tf.train.Example(features = tf.train.Features(\n\n    TypeError: <tf.Tensor 'SerializeTensor:0' shape=() dtype=string> has type SymbolicTensor, but expected one of: bytes\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def serialize_example(inputs: dict):\n",
    "  \"\"\"\n",
    "  Creates a tf.train.Example message ready to be written to a file.\n",
    "  \"\"\"\n",
    "  print(inputs['spatiotemporal'].shape)BytesList\n",
    "  # Create a Features message using tf.train.Example.\n",
    "  example = tf.train.Example(features = tf.train.Features(\n",
    "    feature={\n",
    "      k: tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(v)]))\n",
    "      for k, v in inputs.items()\n",
    "    }\n",
    "  ))\n",
    "  return example.SerializeToString()\n",
    "serialize_example(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature and label tensors...\n",
      "Fetching rainfall configuration...\n",
      "Configuration:  config_v1%2FRainfall_Data_1.txt\n",
      "Rainfall duration:  19\n",
      "The output label tensor will initially be of shape:  [1000, 1000, 19]\n",
      "Creating feature chunks...\n",
      "Creating label chunks...\n",
      "Length of common indices: 66\n",
      "Feature file: gs://test-climateiq-study-area-feature-chunks/Manhattan/scaled_chunk_2_5.npy\n",
      "Label file: gs://test-climateiq-study-area-label-chunks/Manhattan/config_v1/Rainfall_Data_1.txt/2_5.npy\n",
      "Finished creating feature and label tensors...\n",
      "Feature tensor shape: (1000, 1000, 8)\n",
      "Label tensor shape: (19, 1000, 1000)\n",
      "\n",
      "\n",
      "Generating temporal tensors...\n",
      "Creating temporal chunks...\n",
      "Configuration:  config_v1%2FRainfall_Data_1.txt\n",
      "Fetching document: config_v1%2FRainfall_Data_1.txt\n",
      "Loading temporal tensor from gs://test-climateiq-study-area-feature-chunks/rainfall/config_v1/Rainfall_Data_1.npy...\n",
      "Temporal tensor shape:  (864, 6)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid `dataset.` Expected a `tf.data.Dataset` object but got <class 'dict'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset))\n\u001b[1;32m      2\u001b[0m writer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTFRecordWriter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/batch_pred.tfrecord\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/experimental/ops/writers.py:112\u001b[0m, in \u001b[0;36mTFRecordWriter.write\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Writes a dataset to a TFRecord file.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03mAn operation that writes the content of the specified dataset to the file\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m  TypeError: if the elements produced by the dataset are not scalar strings.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, data_types\u001b[38;5;241m.\u001b[39mDatasetV2):\n\u001b[0;32m--> 112\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    113\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `dataset.` Expected a `tf.data.Dataset` object but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m   )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dataset_ops\u001b[38;5;241m.\u001b[39mget_structure(dataset)\u001b[38;5;241m.\u001b[39mis_compatible_with(\n\u001b[1;32m    117\u001b[0m     tensor_spec\u001b[38;5;241m.\u001b[39mTensorSpec([], dtypes\u001b[38;5;241m.\u001b[39mstring)):\n\u001b[1;32m    118\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    119\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `dataset`. Expected a`dataset` that produces scalar \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.string` elements, but got a dataset which produces elements \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_ops\u001b[38;5;241m.\u001b[39mget_legacy_output_shapes(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_ops\u001b[38;5;241m.\u001b[39mget_legacy_output_types(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid `dataset.` Expected a `tf.data.Dataset` object but got <class 'dict'>."
     ]
    }
   ],
   "source": [
    "\n",
    "writer = tf.data.experimental.TFRecordWriter(\"data/batch_pred.tfrecord\")\n",
    "writer.write(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature and label tensors...\n",
      "Fetching rainfall configuration...\n",
      "Configuration:  config_v1%2FRainfall_Data_1.txt\n",
      "Rainfall duration:  19\n",
      "The output label tensor will initially be of shape:  [1000, 1000, 19]\n",
      "Creating feature chunks...\n",
      "Creating label chunks...\n",
      "Length of common indices: 66\n",
      "Feature file: gs://test-climateiq-study-area-feature-chunks/Manhattan/scaled_chunk_4_7.npy\n",
      "Label file: gs://test-climateiq-study-area-label-chunks/Manhattan/config_v1/Rainfall_Data_1.txt/4_7.npy\n",
      "Finished creating feature and label tensors...\n",
      "Feature tensor shape: (1000, 1000, 8)\n",
      "Label tensor shape: (19, 1000, 1000)\n",
      "\n",
      "\n",
      "Generating temporal tensors...\n",
      "Creating temporal chunks...\n",
      "Configuration:  config_v1%2FRainfall_Data_1.txt\n",
      "Fetching document: config_v1%2FRainfall_Data_1.txt\n",
      "Loading temporal tensor from gs://test-climateiq-study-area-feature-chunks/rainfall/config_v1/Rainfall_Data_1.npy...\n",
      "Temporal tensor shape:  (864, 6)\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = next(iter(dataset))\n",
    "inputs = { k: v.numpy().tolist() for k, v in inputs.items() }\n",
    "outfile = 'data/batch_pred_josiahkp.jsonl'\n",
    "\n",
    "# Writing the data to the JSONL file\n",
    "with open(outfile, 'w') as f:\n",
    "    json.dump(inputs, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
